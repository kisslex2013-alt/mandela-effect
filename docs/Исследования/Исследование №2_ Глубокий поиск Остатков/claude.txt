OSINT Strategy: Mandela Effect Residue Hunter
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤

1. üóÑÔ∏è –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ö–∏–≤–∞–º (Internet Archive + –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã)
Internet Archive Wayback Machine API
CDX Server API (–∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Å–Ω–∏–º–∫–æ–≤)
bash# –ë–∞–∑–æ–≤—ã–π endpoint
https://web.archive.org/cdx/search/cdx?url=domain.com&matchType=prefix&output=json

# –ü–æ–∏—Å–∫ –ª–æ–≥–æ—Ç–∏–ø–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞
https://web.archive.org/cdx/search/cdx?url=cocacola.com&from=1996&to=2005&filter=mimetype:image/jpeg&output=json
–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Mandela Effect research:
python# Python example
import requests

def search_wayback_images(domain, start_year, end_year, keyword="logo"):
    params = {
        'url': f'{domain}/*{keyword}*',
        'matchType': 'prefix',
        'from': start_year,
        'to': end_year,
        'filter': ['mimetype:image/jpeg', 'mimetype:image/png', 'mimetype:image/gif'],
        'output': 'json',
        'fl': 'timestamp,original,mimetype,statuscode',
        'collapse': 'digest'  # –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã
    }
    
    response = requests.get('https://web.archive.org/cdx/search/cdx', params=params)
    return response.json()

# –ü—Ä–∏–º–µ—Ä: –∏—Å–∫–∞—Ç—å —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–æ—Ç–∏–ø—ã KitKat
results = search_wayback_images('kitkat.com', '19960101', '20051231', 'logo')
Wayback Machine Availability API
bash# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–Ω–∏–º–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
https://archive.org/wayback/available?url=example.com/logo.png&timestamp=20000101

# Response:
{
  "archived_snapshots": {
    "closest": {
      "available": true,
      "url": "https://web.archive.org/web/20000315.../logo.png",
      "timestamp": "20000315",
      "status": "200"
    }
  }
}
Archive.org Images Search API
pythonimport internetarchive as ia

# –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
search = ia.search_items('collection:vintage_advertisements AND kitkat')

for item in search:
    metadata = ia.get_item(item['identifier'])
    print(f"Found: {metadata.metadata['title']}")
    
    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–≤
    for file in metadata.files:
        if file['format'] in ['JPEG', 'PNG']:
            file.download(destdir='./residue_evidence/')
–ö–ª—é—á–µ–≤—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è Mandela Effect:

vintage_advertisements ‚Äî —Ä–µ–∫–ª–∞–º–∞ 50-90—Ö
ephemera ‚Äî –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (—Ñ–ª–∞–µ—Ä—ã, —É–ø–∞–∫–æ–≤–∫–∏)
television ‚Äî —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¢–í-–ø—Ä–æ–≥—Ä–∞–º–º
magazine_rack ‚Äî –∂—É—Ä–Ω–∞–ª—ã –∏ –∫–∞—Ç–∞–ª–æ–≥–∏
prelinger ‚Äî –ø—Ä–æ–º–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–∞–¥—Ä—ã


2. üñºÔ∏è –í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ (Reverse Image Search APIs)
Google Lens / Vision API (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π)
pythonfrom google.cloud import vision

client = vision.ImageAnnotatorClient()

def find_similar_logos(image_path):
    with open(image_path, 'rb') as image_file:
        content = image_file.read()
    
    image = vision.Image(content=content)
    
    # Web Detection –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö
    response = client.web_detection(image=image)
    
    results = {
        'exact_matches': [img.url for img in response.web_detection.full_matching_images],
        'partial_matches': [img.url for img in response.web_detection.partial_matching_images],
        'visually_similar': [img.url for img in response.web_detection.visually_similar_images]
    }
    
    return results
Limitations:

‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ "Google Lens" —á–µ—Ä–µ–∑ API
‚úÖ Vision API –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
üí∞ $1.50 –∑–∞ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–µ—Ä–≤—ã–µ 1000 –±–µ—Å–ø–ª–∞—Ç–Ω–æ)


TinEye API (–ª—É—á—à–∏–π –¥–ª—è reverse search)
pythonfrom pytineye import TinEyeAPIRequest

# Commercial API (–ø–ª–∞—Ç–Ω—ã–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π)
api = TinEyeAPIRequest(
    api_url='https://api.tineye.com/rest/',
    public_key='YOUR_PUBLIC_KEY',
    private_key='YOUR_PRIVATE_KEY'
)

def search_logo_variations(image_url):
    # –ü–æ–∏—Å–∫ —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ –¥–∞—Ç–µ (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–µ!)
    results = api.search_url(
        url=image_url,
        sort='crawl_date',
        order='asc',
        offset=0,
        limit=100
    )
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö —Å–∞–π—Ç–æ–≤ (1990-2005)
    old_results = [
        match for match in results['matches']
        if '1999' in match['crawl_date'] or '2000' in match['crawl_date']
    ]
    
    return old_results
Pricing: $200/–º–µ—Å—è—Ü –∑–∞ 5000 –ø–æ–∏—Å–∫–æ–≤ (–µ—Å—Ç—å trial)

–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (Free/Open-Source):
Bing Visual Search API
pythonimport requests

def bing_visual_search(image_url):
    endpoint = "https://api.bing.microsoft.com/v7.0/images/visualsearch"
    
    headers = {'Ocp-Apim-Subscription-Key': 'YOUR_KEY'}
    params = {'imgUrl': image_url}
    
    response = requests.post(endpoint, headers=headers, params=params)
    
    # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ (—Å—Ç–∞—Ä—ã–µ —Å–∞–π—Ç—ã)
    results = response.json()
    return [
        img for img in results.get('tags', [])[0].get('actions', [])
        if 'webpage' in img and '1999' in img['webpage'].get('datePublished', '')
    ]
Yandex Images API (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π!)
bash# Reverse Image Search
https://yandex.com/images/search?rpt=imageview&url=IMAGE_URL

# –ù–µ –∏–º–µ–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ API, –Ω–æ –µ—Å—Ç—å –ø–∞—Ä—Å–µ—Ä—ã:
# https://github.com/mvasilkov/yandex-images-api
```

---

## 3. üéØ –ü—Ä–æ–º–ø—Ç-–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –∏ Search Dorks

### **Google Dorks –¥–ª—è Mandela Effect Evidence**

#### **–ü–æ–∏—Å–∫ –ª—é–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Ñ–æ—Ç–æ —É–ø–∞–∫–æ–≤–æ–∫:**
```
# –°—Ç–∞—Ä—ã–µ —Ñ–æ—Ç–æ –Ω–∞ —Ñ–æ—Ä—É–º–∞—Ö
site:reddit.com "kitkat" "wrapper" "1990s" filetype:jpg

# Ebay listings —Å–æ —Å—Ç–∞—Ä—ã–º–∏ —É–ø–∞–∫–æ–≤–∫–∞–º–∏
site:ebay.com "vintage" "kitkat" "packaging" "original"

# Pinterest boards —Å –∞—Ä—Ö–∏–≤–Ω—ã–º–∏ —Ñ–æ—Ç–æ
site:pinterest.com "retro" "logo" "90s" -modern

# Flickr –≥—Ä—É–ø–ø—ã –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤
site:flickr.com/groups "vintage advertising" "kitkat OR monopoly OR fruit loops"

# Internet Archive uploads
site:archive.org "vintage" "commercial" "1990" filetype:mp4
```

#### **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã Google:**
```
# –ü–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ 2000 –≥–æ–¥–∞
"kitkat logo" before:2000-01-01

# –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
"fruit loops" OR "froot loops" after:1995-01-01 before:2005-01-01

# –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Å dorks
site:geocities.ws "coca cola" "logo" before:1999-12-31
```

#### **–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è Mandela Effect:**
```
# –ü–æ–∏—Å–∫ "–æ—à–∏–±–æ–∫" –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ (residue)
"berenstein bears" -"berenstain" site:amazon.com

# VHS covers
"vhs" "original" "cover" "shazaam" OR "kazaam" filetype:jpg

# Newspaper scans
"newspaper" "1990" "monopoly man" "monocle" filetype:pdf

Reddit API –¥–ª—è –ø–æ–∏—Å–∫–∞ residue:
pythonimport praw

reddit = praw.Reddit(
    client_id='YOUR_CLIENT_ID',
    client_secret='YOUR_SECRET',
    user_agent='MandelaResidueBot/1.0'
)

def search_reddit_residue(keyword, subreddits=['MandelaEffect', 'nostalgia', 'vintage']):
    results = []
    
    for subreddit_name in subreddits:
        subreddit = reddit.subreddit(subreddit_name)
        
        # –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
        for submission in subreddit.search(keyword, limit=100, time_filter='all'):
            if submission.url.endswith(('.jpg', '.png', '.gif')):
                results.append({
                    'title': submission.title,
                    'url': submission.url,
                    'created': submission.created_utc,
                    'score': submission.score,
                    'subreddit': subreddit_name
                })
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–µ)
    return sorted(results, key=lambda x: x['created'])

# –ü—Ä–∏–º–µ—Ä
evidence = search_reddit_residue("kitkat logo 90s")

Pinterest API workaround:
Pinterest –Ω–µ –∏–º–µ–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ API, –Ω–æ –µ—Å—Ç—å –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä:
pythonfrom pinterest_scraper import PinterestScraper

scraper = PinterestScraper()

def scrape_vintage_boards(query):
    # –ü–æ–∏—Å–∫ boards —Å –≤–∏–Ω—Ç–∞–∂–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
    boards = scraper.search_boards(query + " vintage retro 90s")
    
    images = []
    for board in boards[:10]:  # –¢–æ–ø-10 boards
        pins = scraper.get_board_pins(board['id'])
        images.extend([
            pin['image_url'] 
            for pin in pins 
            if '1990' in pin.get('description', '') or 'vintage' in pin.get('description', '').lower()
        ])
    
    return images
```

---

## 4. üìö –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∞—Ä—Ö–∏–≤–æ–≤

### **–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∫–ª–∞–º—ã –∏ —É–ø–∞–∫–æ–≤–æ–∫:**

#### **1. AdForum Archive**
```
https://www.adforum.com/creative-work/archive
```
- üé¨ –¢–í-—Ä–µ–∫–ª–∞–º–∞ —Å 1950—Ö
- üîç –ü–æ–∏—Å–∫ –ø–æ –±—Ä–µ–Ω–¥—É + –≥–æ–¥—É
- üí∞ –ß–∞—Å—Ç–∏—á–Ω–æ –ø–ª–∞—Ç–Ω—ã–π

#### **2. Duke's AdAccess**
```
https://repository.duke.edu/dc/adaccess
```
- üì∞ 7000+ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π (1911-1955)
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø
- üì• –í—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ

#### **3. Vintage Ad Browser**
```
https://www.vintageadbrowser.com/
```
- üóìÔ∏è –†–µ–∫–ª–∞–º–∞ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
- üîé –ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (food, toys, etc.)
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ

#### **4. Museum of Brands (Searchable Database)**
```
https://www.museumofbrands.com/
```
- üì¶ 12,000+ —É–ø–∞–∫–æ–≤–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
- üéØ –§–æ–∫—É—Å –Ω–∞ UK/US brands
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è email registration

#### **5. NYPL Digital Collections**
```
https://digitalcollections.nypl.org/
python# API –¥–æ—Å—Ç—É–ø
import requests

def search_nypl_ads(keyword):
    url = "https://api.repo.nypl.org/api/v2/items/search"
    params = {
        'q': f'{keyword} advertisement',
        'field': 'title,date',
        'per_page': 100
    }
    
    response = requests.get(url, params=params)
    return response.json()
```

---

### **VHS/TV Archives:**

#### **1. TV Tropes Screenshot Archive**
```
https://tvtropes.org/pmwiki/pmwiki.php/Main/Screenshots
```

#### **2. RetroVision Archive**
```
https://archive.org/details/retrovision
```
- üìº –û—Ü–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–µ VHS
- üé• –í–∫–ª—é—á–∞–µ—Ç —Ä–µ–∫–ª–∞–º—É –∏ —Ç–∏—Ç—Ä—ã

#### **3. Everything Is Terrible! Archive**
```
https://archive.org/details/everythingisterrible

üé¨ –°—Ç—Ä–∞–Ω–Ω—ã–µ VHS-–Ω–∞—Ö–æ–¥–∫–∏
üî• –ü–æ–ø—É–ª—è—Ä–µ–Ω —Å—Ä–µ–¥–∏ Mandela Effect researchers


üõ°Ô∏è –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–µ–π–∫–æ–≤
–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏ residue:
‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò:

–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:

pythonfrom PIL import Image
from PIL.ExifTags import TAGS

def check_image_authenticity(image_path):
    image = Image.open(image_path)
    exifdata = image.getexif()
    
    metadata = {}
    for tag_id, value in exifdata.items():
        tag = TAGS.get(tag_id, tag_id)
        metadata[tag] = value
    
    # Red flags
    red_flags = []
    
    # 1. –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–∑–∂–µ —á–µ–º "—Å–æ–±—ã—Ç–∏–µ Mandela Effect"
    if 'DateTime' in metadata:
        if int(metadata['DateTime'][:4]) > 2010:
            red_flags.append("Image created after ME event")
    
    # 2. –°–ª–µ–¥—ã Photoshop
    if 'Software' in metadata:
        if 'Adobe Photoshop' in metadata['Software']:
            red_flags.append("Edited in Photoshop")
    
    # 3. –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è 90-—Ö
    if image.width > 1920 or image.height > 1080:
        red_flags.append("Resolution too high for era")
    
    return {
        'metadata': metadata,
        'red_flags': red_flags,
        'authenticity_score': 100 - (len(red_flags) * 30)
    }

–ò—Å—Ç–æ—á–Ω–∏–∫:

‚úÖ –°—Ç–∞—Ä—ã–µ —Ñ–æ—Ä—É–º—ã (phpBB, 2000-2005)
‚úÖ Wayback Machine snapshots
‚úÖ eBay listings —Å —Ñ–æ—Ç–æ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
‚ùå –ù–µ–¥–∞–≤–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å–∞–π—Ç—ã
‚ùå –ò–º–∏–¥–∂–±–æ—Ä–¥—ã –±–µ–∑ –º–æ–¥–µ—Ä–∞—Ü–∏–∏


–ö–æ–Ω—Ç–µ–∫—Å—Ç:

‚úÖ –°–ª—É—á–∞–π–Ω–æ–µ —Ñ–æ—Ç–æ (—Å–µ–º–µ–π–Ω—ã–π –∞–ª—å–±–æ–º, —Ñ–æ–Ω)
‚úÖ –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (VHS, —Å–∫–∞–Ω—ã)
‚ùå –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
‚ùå –§–æ—Ç–æ "—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ" –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ø–æ—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç


–¶–∏—Ñ—Ä–æ–≤–æ–π —Å–ª–µ–¥:

pythondef check_reverse_image_history(image_url):
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º TinEye –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
    results = tineye_api.search_url(image_url, sort='crawl_date', order='asc')
    
    first_appearance = results['matches'][0]['crawl_date'] if results['matches'] else None
    
    # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ –ü–û–°–õ–ï –Ω–∞—á–∞–ª–∞ –¥–µ–±–∞—Ç–æ–≤ –æ Mandela Effect ‚Äî suspicious
    if first_appearance and int(first_appearance[:4]) > 2010:
        return {
            'status': 'suspicious',
            'reason': 'First appeared online after ME discussions began'
        }
    
    return {'status': 'potentially_authentic'}

Machine Learning –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏:
pythonimport tensorflow as tf
from transformers import CLIPProcessor, CLIPModel

# CLIP model –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def detect_anachronisms(image, claimed_era):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç—ã, –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–∞—è–≤–ª–µ–Ω–Ω–æ–π —ç–ø–æ—Ö–µ"""
    
    # –û–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ 90-—Ö
    modern_objects = [
        "smartphone", "iphone", "modern car", "LED screen", 
        "QR code", "USB drive", "modern laptop"
    ]
    
    inputs = processor(text=modern_objects, images=image, return_tensors="pt", padding=True)
    outputs = model(**inputs)
    
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)
    
    # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ > 30% ‚Äî —Ñ–µ–π–∫
    if probs.max() > 0.3:
        detected = modern_objects[probs.argmax()]
        return {
            'authentic': False,
            'reason': f'Detected anachronism: {detected}'
        }
    
    return {'authentic': True}

ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline
–ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ residue:
pythonclass MandelaResidueHunter:
    def __init__(self):
        self.sources = {
            'wayback': WaybackSearcher(),
            'tineye': TinEyeSearcher(),
            'reddit': RedditScraper(),
            'archive_org': ArchiveOrgSearcher()
        }
    
    def hunt(self, mandela_effect_case):
        """
        mandela_effect_case = {
            'item': 'KitKat logo',
            'disputed_element': 'hyphen',
            'era': '1990-2005',
            'keywords': ['kitkat', 'kit-kat', 'wrapper']
        }
        """
        
        all_evidence = []
        
        # 1. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        for source_name, searcher in self.sources.items():
            results = searcher.search(
                keywords=mandela_effect_case['keywords'],
                date_range=mandela_effect_case['era']
            )
            
            for result in results:
                # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç–∏
                authenticity = self.validate_evidence(result)
                
                # 3. –í–∏–∑—É–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
                if authenticity['score'] > 70:
                    visual_match = self.check_disputed_element(
                        result['image_url'],
                        mandela_effect_case['disputed_element']
                    )
                    
                    if visual_match:
                        all_evidence.append({
                            'source': source_name,
                            'url': result['url'],
                            'image_url': result['image_url'],
                            'date': result['date'],
                            'authenticity_score': authenticity['score'],
                            'confidence': visual_match['confidence']
                        })
        
        # 4. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        return sorted(all_evidence, key=lambda x: x['authenticity_score'], reverse=True)
    
    def validate_evidence(self, result):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        checks = [
            self.check_metadata(result['image_url']),
            self.check_source_age(result['url']),
            self.check_reverse_image_history(result['image_url']),
            self.detect_anachronisms(result['image_url'], result.get('claimed_era'))
        ]
        
        total_score = sum(check['score'] for check in checks) / len(checks)
        return {'score': total_score, 'checks': checks}

üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ API
API/ToolCostRate LimitBest ForDate FilterWayback Machine CDXFreeNone–°—Ç–∞—Ä—ã–µ —Å–∞–π—Ç—ã‚úÖ –¢–æ—á–Ω—ã–µ –¥–∞—Ç—ãTinEye$200/mo5000/moReverse search‚úÖ Crawl dateGoogle Vision$1.50/1K1800/min–ü–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è‚ùå –ù–µ—ÇBing VisualFree tier1000/moReverse search‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æReddit APIFree60/minUser-generated‚úÖ Post dateArchive.org ImagesFreeNoneVintage content‚úÖ –¢–æ—á–Ω—ã–µ –¥–∞—Ç—ã

üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –Ω–∞—á–∞–ª–∞:
Week 1: Setup Infrastructure

‚úÖ Wayback Machine CDX API (free, unlimited)
‚úÖ Archive.org Collections API (free)
‚úÖ Reddit PRAW (free, 60 req/min)

Week 2: Add Paid Services (if budget allows)

üí∞ TinEye API trial ($200/mo)
üí∞ Google Vision API (free tier: 1000/mo)

Week 3: Automation

ü§ñ Scheduled scraping (daily)
üîç ML-based validation
üìä Evidence dashboard


Total setup cost: $0-200/month
Expected residue finds: 50-200/month (depending on ME case popularity)